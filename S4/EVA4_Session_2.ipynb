{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA4 - Session 2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amar-naik/EVA/blob/master/S4/EVA4_Session_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m2JWFliFfKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch # PyTorch is a python package that provides two high-level features: - Tensor computation (like numpy) with strong GPU acceleration - Deep. Neural Networks built on a tape-based autograd system\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F #This module contains all the functions in the torch.nn library (whereas other parts of the library contain classes). As well as a wide range of loss and activation functions, you’ll also find here some convenient functions for creating neural nets, such as pooling functions.\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms # The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision\n",
        "#torch.nn\n",
        "#Module: creates a callable which behaves like a function, but can also contain state(such as neural net layer weights). It knows what Parameter (s) it contains and can zero all their gradients, loop through them for weight updates, etc.\n",
        "#Parameter: a wrapper for a tensor that tells a Module that it has weights that need updating during backprop. Only tensors with the requires_grad attribute set are updated\n",
        "#functional: a module(usually imported into the F namespace by convention) which contains activation functions, loss functions, etc, as well as non-stateful versions of layers such as convolutional and linear layers.\n",
        "#torch.optim: Contains optimizers such as SGD, which update the weights of Parameter during the backward step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_Cx9q2QFgM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We use the Conv2d layer because our image data is two dimensional.\n",
        "#The padding argument indicates how much  padding is added to the edges of the data during computation.\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 8, 3, padding=1) #\n",
        "        self.conv1_BN = nn.BatchNorm2d(8)\n",
        "        self.conv2 = nn.Conv2d(8, 12, 3, padding=1) \n",
        "        self.conv2_BN = nn.BatchNorm2d(12)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2) \n",
        "        self.drop_out = nn.Dropout(0.25)\n",
        "        self.conv3 = nn.Conv2d(12, 16, 3, padding=1) \n",
        "        self.conv3_BN = nn.BatchNorm2d(16)\n",
        "        self.conv4 = nn.Conv2d(16, 18, 3, padding=1) \n",
        "        self.conv4_BN = nn.BatchNorm2d(18)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)  \n",
        "        self.drop_out = nn.Dropout(0.25)\n",
        "        self.conv5 = nn.Conv2d(18, 24, 3) \n",
        "        self.conv5_BN = nn.BatchNorm2d(24)\n",
        "        self.conv6 = nn.Conv2d(24, 28, 3)\n",
        "        self.conv6_BN = nn.BatchNorm2d(28)\n",
        "        self.conv7 = nn.Conv2d(28, 17, 3) \n",
        "        self.conv7_BN = nn.BatchNorm2d(17)\n",
        "        #self.conv7 = nn.Dropout(0.25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv2_BN(self.conv2(F.relu(self.conv1_BN(self.conv1(x)))))))\n",
        "        x = self.pool2(F.relu(self.conv4_BN(self.conv4(F.relu(self.conv3_BN(self.conv3(x)))))))\n",
        "        x = F.relu(self.conv6_BN(self.conv6(F.relu(self.conv5_BN(self.conv5(x))))))\n",
        "        x = F.relu(self.conv7_BN(self.conv7(x)))\n",
        "        #x = F.relu(self.drop_out(x))\n",
        "        x = x.view(-1, 17) # expecting a output of 1X1 \n",
        "        #x = x.flatten(0,1)\n",
        "        return F.log_softmax(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xdydjYTZFyi3",
        "outputId": "2b6ee910-d84e-4f9e-e28f-ad8e8abd140a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "!pip install torchsummary #you can get exact Keras representation, using pytorch-summary package\n",
        "from torchsummary import summary\n",
        "use_cuda = torch.cuda.is_available() # This package adds support for CUDA tensor types, that implement the same function as CPU tensors, \n",
        "#but they utilize GPUs for computation\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = Net().to(device) ## to(device) moves the model weights to GPU.\n",
        "summary(model, input_size=(1, 28, 28)) # input image size"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 8, 28, 28]              80\n",
            "       BatchNorm2d-2            [-1, 8, 28, 28]              16\n",
            "            Conv2d-3           [-1, 12, 28, 28]             876\n",
            "       BatchNorm2d-4           [-1, 12, 28, 28]              24\n",
            "         MaxPool2d-5           [-1, 12, 14, 14]               0\n",
            "            Conv2d-6           [-1, 16, 14, 14]           1,744\n",
            "       BatchNorm2d-7           [-1, 16, 14, 14]              32\n",
            "            Conv2d-8           [-1, 18, 14, 14]           2,610\n",
            "       BatchNorm2d-9           [-1, 18, 14, 14]              36\n",
            "        MaxPool2d-10             [-1, 18, 7, 7]               0\n",
            "           Conv2d-11             [-1, 24, 5, 5]           3,912\n",
            "      BatchNorm2d-12             [-1, 24, 5, 5]              48\n",
            "           Conv2d-13             [-1, 28, 3, 3]           6,076\n",
            "      BatchNorm2d-14             [-1, 28, 3, 3]              56\n",
            "           Conv2d-15             [-1, 17, 1, 1]           4,301\n",
            "      BatchNorm2d-16             [-1, 17, 1, 1]              34\n",
            "================================================================\n",
            "Total params: 19,845\n",
            "Trainable params: 19,845\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.38\n",
            "Params size (MB): 0.08\n",
            "Estimated Total Size (MB): 0.46\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqTWLaM5GHgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.manual_seed(1) # Sets the seed for generating random numbers. Returns a torch.Generator object. It is recommended to set a large seed, i.e. a number that has a good balance of 0 and 1 bits. Avoid having many 0 bits in the seed\n",
        "batch_size = 128\n",
        "#transforms.Compose just clubs all the transforms provided to it. So, all the transforms in the transforms.Compose are applied to the input one by one\n",
        "#transforms.ToTensor(): This just converts your input image to PyTorch tensor.\n",
        "#transforms.Normalize((0.1307,), (0.3081,)): typically used data scaling and these values (mean and std) must have been \n",
        "#precomputed for your dataset. Changing these values is also not advised.\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "#Combines a dataset and a sampler, and provides an iterable over the given dataset. The :class:`~torch.utils.data.DataLoader` \n",
        "#supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and \n",
        "#optional automatic batching (collation) and memory pinning.\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fDefDhaFlwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm # add progress bars to Python code is with tqdm\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad() #Clears the gradients of all optimized torch.Tensor s.\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target) #The negative log likelihood loss\n",
        "        loss.backward() ## Back Propagation\n",
        "        optimizer.step() ## Gradient Descent\n",
        "        #Recall that when initializing optimizer you explicitly tell it what parameters (tensors) of the model it should be updating. \n",
        "        #The gradients are \"stored\" by the tensors themselves (they have a grad and a requires_grad attributes) once you call backward() on the loss. \n",
        "        #After computing the gradients for all tensors in the model, calling optimizer.step() makes the optimizer iterate over all parameters (tensors) \n",
        "        #it is supposed to update and use their internally stored grad to update their values.\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    # since we need to print 99.4 Accuracy we need atleast one values after radix to be printed. Hence changing .0f in accuracy to .2f\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMWbLWO6FuHb",
        "colab_type": "code",
        "outputId": "c907b87e-2ffa-4d54-907a-384cbc20da3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.02, momentum=0.7)\n",
        "\n",
        "for epoch in range(0, 20):\n",
        "    print (\"epoch\", epoch)\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "loss=0.1380969136953354 batch_id=468: 100%|██████████| 469/469 [00:14<00:00, 32.88it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.1365, Accuracy: 9798/10000 (97.98%)\n",
            "\n",
            "epoch 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.10474128276109695 batch_id=468: 100%|██████████| 469/469 [00:13<00:00, 35.50it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0544, Accuracy: 9895/10000 (98.95%)\n",
            "\n",
            "epoch 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.02398555539548397 batch_id=468: 100%|██████████| 469/469 [00:14<00:00, 32.87it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0426, Accuracy: 9903/10000 (99.03%)\n",
            "\n",
            "epoch 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.0184024628251791 batch_id=468: 100%|██████████| 469/469 [00:13<00:00, 33.84it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0421, Accuracy: 9924/10000 (99.24%)\n",
            "\n",
            "epoch 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.0529310405254364 batch_id=468: 100%|██████████| 469/469 [00:14<00:00, 31.04it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0337, Accuracy: 9926/10000 (99.26%)\n",
            "\n",
            "epoch 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.10895514488220215 batch_id=468: 100%|██████████| 469/469 [00:13<00:00, 35.15it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0305, Accuracy: 9930/10000 (99.30%)\n",
            "\n",
            "epoch 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.027787959203124046 batch_id=468: 100%|██████████| 469/469 [00:13<00:00, 33.81it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0313, Accuracy: 9934/10000 (99.34%)\n",
            "\n",
            "epoch 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.006654143333435059 batch_id=468: 100%|██████████| 469/469 [00:14<00:00, 36.31it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0389, Accuracy: 9900/10000 (99.00%)\n",
            "\n",
            "epoch 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.011768211610615253 batch_id=468: 100%|██████████| 469/469 [00:14<00:00, 31.37it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0293, Accuracy: 9925/10000 (99.25%)\n",
            "\n",
            "epoch 9\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.011778314597904682 batch_id=468: 100%|██████████| 469/469 [00:14<00:00, 33.21it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0287, Accuracy: 9920/10000 (99.20%)\n",
            "\n",
            "epoch 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.019009670242667198 batch_id=468: 100%|██████████| 469/469 [00:13<00:00, 34.05it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0279, Accuracy: 9923/10000 (99.23%)\n",
            "\n",
            "epoch 11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.019745951518416405 batch_id=468: 100%|██████████| 469/469 [00:14<00:00, 33.20it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0269, Accuracy: 9934/10000 (99.34%)\n",
            "\n",
            "epoch 12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.019543210044503212 batch_id=468: 100%|██████████| 469/469 [00:14<00:00, 32.16it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0256, Accuracy: 9930/10000 (99.30%)\n",
            "\n",
            "epoch 13\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.02384628914296627 batch_id=468: 100%|██████████| 469/469 [00:14<00:00, 33.16it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0246, Accuracy: 9935/10000 (99.35%)\n",
            "\n",
            "epoch 14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.011026074178516865 batch_id=468: 100%|██████████| 469/469 [00:14<00:00, 33.22it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0239, Accuracy: 9937/10000 (99.37%)\n",
            "\n",
            "epoch 15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.00301940250210464 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 30.81it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0253, Accuracy: 9930/10000 (99.30%)\n",
            "\n",
            "epoch 16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.008526623249053955 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 29.79it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0239, Accuracy: 9934/10000 (99.34%)\n",
            "\n",
            "epoch 17\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.0026768248062580824 batch_id=468: 100%|██████████| 469/469 [00:15<00:00, 31.18it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0249, Accuracy: 9940/10000 (99.40%)\n",
            "\n",
            "epoch 18\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.005864972714334726 batch_id=468: 100%|██████████| 469/469 [00:14<00:00, 33.30it/s]\n",
            "  0%|          | 0/469 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0268, Accuracy: 9924/10000 (99.24%)\n",
            "\n",
            "epoch 19\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loss=0.012511556036770344 batch_id=468: 100%|██████████| 469/469 [00:14<00:00, 32.06it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0234, Accuracy: 9939/10000 (99.39%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So5uk4EkHW6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}